{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add .. to sys.path\n",
    "import sys\n",
    "sys.path.append('..')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/analytics-vidhya/airbnb-data-exploration-analysis-and-feature-engineering-edbb47bf115"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputation is the process of filling in missing values in a dataset. Here are some common imputation strategies for machine learning:\n",
    "\n",
    "1. Mean/Median/Mode imputation: Replace missing values with the mean, median or mode of the non-missing values in the same column.\n",
    "\n",
    "2. Regression imputation: Use a regression model to predict missing values based on other features in the dataset.\n",
    "\n",
    "3. K-Nearest Neighbors (KNN) imputation: Find the K nearest neighbors to the observation with missing values and use their values to impute the missing value.\n",
    "\n",
    "4. Multiple Imputation by Chained Equations (MICE): MICE is an iterative imputation method that imputes missing values multiple times using a regression model and combines the results to create a final imputed dataset.\n",
    "\n",
    "5. Random Forest imputation: Use a random forest model to predict missing values based on other features in the dataset.\n",
    "\n",
    "6. Deep Learning imputation: Use a deep learning model to predict missing values based on other features in the dataset.\n",
    "\n",
    "The choice of imputation strategy depends on the nature of the missing data and the specific requirements of the machine learning problem. It is important to carefully consider the implications of each imputation strategy and evaluate the performance of the resulting model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOs and options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- feature eng\n",
    "- feature scaling\n",
    "- target transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sklearn.dummy.DummyClassifier\n",
    "- sklearn.dummy.DummyRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:  \n",
    "replaced `from collections import Iterable` with `from collections.abc import Iterable` in `~/anaconda3/envs/cheatsheet/lib/python3.10/site-packages/causalgraphicalmodels/cgm.py` because of python 3.10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"190pt\" height=\"260pt\"\n",
       " viewBox=\"0.00 0.00 189.89 260.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 256)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-256 185.89,-256 185.89,4 -4,4\"/>\n",
       "<!-- sprinkler -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>sprinkler</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"52.65\" cy=\"-162\" rx=\"52.79\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"52.65\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">sprinkler</text>\n",
       "</g>\n",
       "<!-- wet -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>wet</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"102.65\" cy=\"-90\" rx=\"27.9\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"102.65\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">wet</text>\n",
       "</g>\n",
       "<!-- sprinkler&#45;&gt;wet -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>sprinkler&#45;&gt;wet</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M64.49,-144.41C70.69,-135.74 78.38,-124.97 85.23,-115.38\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"88.26,-117.16 91.23,-106.99 82.56,-113.09 88.26,-117.16\"/>\n",
       "</g>\n",
       "<!-- slippery -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>slippery</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"102.65\" cy=\"-18\" rx=\"48.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"102.65\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">slippery</text>\n",
       "</g>\n",
       "<!-- rain -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>rain</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"152.65\" cy=\"-162\" rx=\"29.5\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"152.65\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">rain</text>\n",
       "</g>\n",
       "<!-- rain&#45;&gt;wet -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>rain&#45;&gt;wet</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M141.3,-145.12C134.97,-136.26 126.96,-125.04 119.87,-115.12\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"122.61,-112.92 113.95,-106.82 116.91,-116.99 122.61,-112.92\"/>\n",
       "</g>\n",
       "<!-- wet&#45;&gt;slippery -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>wet&#45;&gt;slippery</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M102.65,-71.7C102.65,-63.98 102.65,-54.71 102.65,-46.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"106.15,-46.1 102.65,-36.1 99.15,-46.1 106.15,-46.1\"/>\n",
       "</g>\n",
       "<!-- season -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>season</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"102.65\" cy=\"-234\" rx=\"42.49\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"102.65\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">season</text>\n",
       "</g>\n",
       "<!-- season&#45;&gt;sprinkler -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>season&#45;&gt;sprinkler</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M90.8,-216.41C84.78,-207.99 77.35,-197.58 70.65,-188.2\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"73.42,-186.06 64.76,-179.96 67.72,-190.13 73.42,-186.06\"/>\n",
       "</g>\n",
       "<!-- season&#45;&gt;rain -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>season&#45;&gt;rain</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M114.49,-216.41C120.69,-207.74 128.38,-196.97 135.23,-187.38\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"138.26,-189.16 141.23,-178.99 132.56,-185.09 138.26,-189.16\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7fa0e7e45630>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from causalgraphicalmodels import CausalGraphicalModel\n",
    "sprinkler = CausalGraphicalModel(\n",
    "    nodes=[\"season\", \"rain\", \"sprinkler\", \"wet\", \"slippery\"],\n",
    "    edges=[\n",
    "        (\"season\", \"rain\"), \n",
    "        (\"season\", \"sprinkler\"), \n",
    "        (\"rain\", \"wet\"),\n",
    "        (\"sprinkler\", \"wet\"), \n",
    "        (\"wet\", \"slippery\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# draw return a graphviz `dot` object, which jupyter can render\n",
    "sprinkler.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/ijmbarr/causalgraphicalmodels/blob/master/notebooks/cgm-examples.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@whystudying/causal-inference-with-python-causal-graphs-ef2f3a52c266"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Normality test (or shapiro)\n",
    "   from scipy.stats import normaltest   \n",
    "   The null hypothesis of the test (H0), or the default expectation, is that the statistic describes a normal distribution.   \n",
    "      \n",
    "- T test homoelasticity\n",
    "   value, pvalue = ttest_ind(values1, values2, equal_var=True)  \n",
    "   The null hypothesis of the test (H0) or the default expectation is that both samples were drawn from the same population\n",
    "- T test heteroelasticity  \n",
    "    value, pvalue = ttest_ind(values1, values2, equal_var=False)  \n",
    "- Compare means for non-gaussian distributions: Kolmogorov-Smirnov\n",
    "    from scipy.stats import ks_2samp  \n",
    "    This is a two-sided test for the null hypothesis that 2 independent samples are drawn from the same continuous distribution.  \n",
    "    less statistical power and may require large samples  \n",
    "- Compare means for non-gaussian with small samples: Mann-Whitney / Wilcoxon\n",
    "    from scipy.stats import mannwhitneyu\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary classification:\n",
    "- from scipy.stats import chi2_contingency\n",
    "    stat, p_value = chi2_contingency(table, correction=False)  \n",
    "    pd.crosstab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chi2 test:\n",
    "- determine whether there is a significant association between two cateorical variables\n",
    "- stat, p_value, dof, expected = chi2_contingency(table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANOVA\n",
    "- from scipy.stats import f_oneway\n",
    "-  null hypothesis no significant difference between means of samples\n",
    "- assumes normality and equal variances\n",
    "- use kruskal-wallis test if hypothesis broken: from scipy.stats import kruskal (non-parametric on median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SHAP is about local interpretability of a predictive model\n",
    "\n",
    "- the “game” is reproducing the outcome of the model,\n",
    "- the “players” are the features included in the model.\n",
    "\n",
    "What Shapley does is quantifying the contribution that each player brings to the game.  \n",
    "What SHAP does is quantifying the contribution that each feature brings to the prediction made by the model.\n",
    "\n",
    "Idea is to train on model for each combinaison of feature (F features, 2 ^ F models). Approximations and sampling in actual implementation.  \n",
    "The marginal effect of one feature is the sum of all contribution where the feature is added (diff vs same model without the feature), each weighted by $(f * \\binom{F}{f}) ^ {-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explainer = shap.Explainer(model, X)  \n",
    "shap_values = explainer(X)  \n",
    "shap.plots.beeswarm(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANOVA\n",
    "- from scipy.stats import f_oneway\n",
    "-  null hypothesis no significant difference between means of samples\n",
    "- assumes normality and equal variances\n",
    "- use kruskal-wallis test if hypothesis broken: from scipy.stats import kruskal (non-parametric on median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANOVA\n",
    "- from scipy.stats import f_oneway\n",
    "-  null hypothesis no significant difference between means of samples\n",
    "- assumes normality and equal variances\n",
    "- use kruskal-wallis test if hypothesis broken: from scipy.stats import kruskal (non-parametric on median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features interactions in model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Occur when the combined effect of two or more features on a model’s prediction is greater than the sum of their individual effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate partial dependence for a specific feature, we follow these steps:\n",
    "\n",
    "- Fix the value of the feature of interest.\n",
    "- For each instance in the dataset, replace the actual value of the feature with the fixed value.\n",
    "- Use the model to make predictions for the modified dataset.\n",
    "- Average the predictions to obtain the partial dependence value for that feature.\n",
    "- Repeat this process for every value across every feature of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shap.plots.partial_dependence(\n",
    "    \"AveOccup\", model.predict, X100, ice=False,\n",
    "    model_expected_value=True, feature_expected_value=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.gopenai.com/uncover-feature-interactions-in-your-machine-learning-models-with-two-way-partial-dependence-plots-3bc6bff00078"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFE using error participation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression: \n",
    "- the *Prediction Contribution* of a feature is equal to the mean of the absolute SHAP values of that feature\n",
    "- doesn’t tell anything about the feature’s performance\n",
    "\n",
    "Compute *the Error Contribution*:\n",
    "- remove effect of feature on prediction: `y_pred_wo_feature = shap_values.apply(lambda feature: y_pred - feature)`\n",
    "- compute prediction error without the feature: `abs_error_wo_feature = y_pred_wo_feature.apply(lambda feature: (y_true - feature).abs())`\n",
    "- compute the difference between the errors of the full model and the errors we would obtain without the feature: `error_diff = abs_error_wo_feature.apply(lambda feature: abs_error - feature)`\n",
    "\n",
    "If this number is:\n",
    "\n",
    "- negative, then the presence of the feature leads to a reduction in the prediction error, so the feature works well for that observation!\n",
    "- positive, then the presence of the feature leads to an increase in the prediction error, so the feature is bad for that observation.\n",
    "\n",
    "Finaly: `error_contribution = error_diff.mean()`\n",
    "\n",
    "Plot Error Contribution = f(Prediction Contribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification:\n",
    "- Use sigmoid function on the sum of shap values to get a probability\n",
    "- prediction contribution is the same\n",
    "- remove feature shap value before suming the shap values and applying sigmoid (get proba without the feature)\n",
    "- compute individual log loss between those proba and the true class\n",
    "- now compute individual log loss with the actual prediction and substract the previous value\n",
    "- error contribution of the feature is the mean of all diff for a feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/which-features-are-harmful-for-your-classification-model-6227859a44a6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayessian optimization explained:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- build a probability model of the objective function and use it to select the most promising hyperparameters to evaluate in the true objective function.\n",
    "- create a surrogate objective function $P(score | hyperparameters)$ easier to optimize. Find a promosing set of hyperparameters, use it on real objective function and update the surrogate function accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost hyperparameters explained:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/10-confusing-xgboost-hyperparameters-and-how-to-tune-them-like-a-pro-in-2023-e305057f546"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cheatsheet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
